{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will load de embeddings model, the vector db generated in the ingestion step to continue with the app.\n",
    "\n",
    "Once it's finished, we'll generate the .py associated leaving only the code needed for the production version of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TALIGENT\\Desktop\\Proyectos\\LLM-Zoomcamp\\MessiXpert-RAG\\messiXpert-RAG-venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embeddings model\n",
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize the Elasticsearch client\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticSearch Index names\n",
    "index_name_cosine = \"messixpert_cosine\"\n",
    "index_name_dot_product = \"messixpert_dot_product\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(user_query, es_client, index):\n",
    "    query = {\n",
    "        \"size\": 5,  \n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"content\": user_query  # Search in the content field\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = es_client.search(index=index, body=query)\n",
    "    \n",
    "    return results[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(user_query, es_client, index):\n",
    "\n",
    "    user_query = embeddings_model.encode(user_query)\n",
    "\n",
    "    query = {\n",
    "        \"k\": 5,  \n",
    "        \"field\": \"content_embeddings\",\n",
    "        \"query_vector\": user_query\n",
    "    }\n",
    "\n",
    "    results = es_client.search(index=index, knn=query)\n",
    "\n",
    "    return results[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_content(answers):\n",
    "\n",
    "    retrieved_texts = [hit[\"_source\"][\"content\"] for hit in answers]\n",
    "\n",
    "    return retrieved_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results_text_list):\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "You are an expert biographer on the life and career of Lionel Messi, with deep knowledge of his entire history and statistics. \n",
    "Your task is to answer users questions based solely on the context provided to you. \n",
    "Answer respectfully and in a warm way, as if you are an assistant.\n",
    "Answer in the same language that you are asked, if you are asked in english then answer in english, but if you are asked in spanish then answer in spanish.\n",
    "Use only the data from the context to answer the question. \n",
    "If you cannot answer the question with the provided information, respond: “I’m sorry, but I don’t have enough information to answer that. Is there anything else I can help you with?”\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: \n",
    "{question}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for text in search_results_text_list:\n",
    "        context = context + f\"{text}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(response):\n",
    "\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "\n",
    "    input_tokens_cost_per_1k = 0.00015\n",
    "    output_tokens_cost_per_1k = 0.0006\n",
    "\n",
    "    input_tokens_cost = input_tokens_cost_per_1k * (input_tokens / 1000)\n",
    "    output_tokens_cost = output_tokens_cost_per_1k * (output_tokens / 1000)\n",
    "    total_cost = input_tokens_cost + output_tokens_cost\n",
    "\n",
    "    print(\"------------------------------------\")\n",
    "    print(f\"Input Tokens: {input_tokens}       Cost: ${input_tokens_cost:.8f}\")\n",
    "    print(f\"Completion Tokens: {output_tokens}       Cost: ${output_tokens_cost:.8f}\")\n",
    "    print(f\"Total Cost: ${total_cost:.8f}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate_answer(prompt, open_ai_client):\n",
    "    response = open_ai_client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    calculate_cost(response)\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, search_function, es_client, index_name, open_ai_client):\n",
    "\n",
    "    top_k_chunks = search_function(user_query=question, es_client=es_client, index=index_name)\n",
    "\n",
    "    answers = get_answers_content(top_k_chunks)\n",
    "    \n",
    "    builded_prompt = build_prompt(query=question, search_results_text_list=answers)\n",
    "    \n",
    "    answer = llm_generate_answer(builded_prompt, open_ai_client=open_ai_client)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAI_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Cuántos hermanos tiene Messi?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Input Tokens: 1139       Cost: $0.00017085\n",
      "Completion Tokens: 33       Cost: $0.00001980\n",
      "Total Cost: $0.00019065\n",
      "------------------------------------\n",
      "Lionel Messi tiene tres hermanos: dos mayores, Rodrigo y Matías, y una hermana menor, María Sol. ¿Hay algo más en lo que pueda ayudarte?\n"
     ]
    }
   ],
   "source": [
    "answer_knn = generate_answer(question=question,\n",
    "                             search_function=knn_search, \n",
    "                             es_client=es_client, \n",
    "                             index_name=index_name_cosine,\n",
    "                             open_ai_client=OpenAI_client)\n",
    "\n",
    "print(answer_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Input Tokens: 2157       Cost: $0.00032355\n",
      "Completion Tokens: 37       Cost: $0.00002220\n",
      "Total Cost: $0.00034575\n",
      "------------------------------------\n",
      "Lionel Messi tiene tres hermanos: dos hermanos mayores, Rodrigo y Matías, y una hermana menor, María Sol. Si necesitas saber algo más, estaré encantado de ayudarte.\n"
     ]
    }
   ],
   "source": [
    "answer_text = generate_answer(question=question,\n",
    "                             search_function=text_search, \n",
    "                             es_client=es_client, \n",
    "                             index_name=index_name_cosine,\n",
    "                             open_ai_client=OpenAI_client)\n",
    "\n",
    "print(answer_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "messiXpert-RAG-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
